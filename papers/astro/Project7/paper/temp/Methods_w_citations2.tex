\documentclass[twocolumn]{aastex631}

\usepackage{amsmath}
\usepackage{multirow}
\usepackage{natbib}
\usepackage{graphicx} 
\usepackage{aas_macros}

\begin{document}

\subsection{Data Acquisition and Preprocessing}
\subsubsection{Dataset Description}
The foundation of our analysis rests on a dataset comprising 1000 dark matter halo merger trees. These trees were extracted from N-body simulations, capturing the hierarchical formation history of dark matter halos. Each merger tree is structured as a directed acyclic graph, where nodes represent dark matter halos at different points in time, and edges signify merger events between halos. The dataset is conveniently stored in the PyTorch Geometric format, a library designed for handling graph-structured data. Each `Data` object within the dataset encapsulates the following crucial information:
\begin{itemize}
    \item \textbf{Node Features (`x`)}: A matrix of shape `[num\_nodes, 4]` containing four key properties for each halo (node): the base-10 logarithm of the halo mass (log10(mass)), the base-10 logarithm of the halo concentration (log10(concentration)), the base-10 logarithm of the maximum circular velocity (log10(Vmax)), and the cosmological scale factor at the time the halo existed.
    \item \textbf{Edge Index (`edge\_index`)}: A matrix of shape `[2, num\_edges]` that defines the graph's connectivity, specifying the source and target nodes for each merger event (edge).
    \item \textbf{Edge Attributes (`edge_attr`)}: A matrix of shape `[num_edges, 1]` that stores a single feature associated with each merger event. This feature's precise nature will be carefully examined during exploratory data analysis (EDA), and if it proves insufficient for our purposes, we will compute astrophysically motivated edge features from the node properties.
    \item \textbf{Graph-Level Labels (`y`)}: A matrix of shape `[1, 2]` containing the cosmological parameters associated with the simulation from which the merger tree was extracted: the matter density parameter ($\Omega_m$) and the amplitude of matter fluctuations ($\sigma_8$). These are the target variables for our regression models.
    \item \textbf{Number of Nodes (`num_nodes`)}: The total number of halos (nodes) in the merger tree.
    \item \textbf{Latin Hypercube ID (`lh_id`)}: An identifier indicating the specific Latin Hypercube simulation from which the merger tree originated. This is crucial for ensuring simulation-aware data splitting.
    \item \textbf{Node Halo ID (`node_halo_id`)}: Unique identifiers for each halo (node) within the merger tree.
\end{itemize}

\subsubsection{Exploratory Data Analysis}
Prior to feature engineering and model training, we conducted a comprehensive exploratory data analysis (EDA) to gain a thorough understanding of the dataset's characteristics.
This involved examining the distributions of node features, target variables, and graph structural properties \citep{haghighi2023analyzingastronomicaldatamachine}.
The EDA process included:
\begin{enumerate}
    \item \textbf{Node Feature Distributions}: Analyzing the distributions of the four node features (log10(mass), log10(concentration), log10(Vmax), scale factor) across all nodes in all 1000 merger trees. This step is critical for identifying potential outliers, understanding the ranges of feature values, and informing subsequent normalization procedures.
    \item \textbf{Graph Target Variable Distributions}: Examining the distributions of the cosmological parameters ($\Omega_m$ and $\sigma_8$). Given that the dataset comprises 1000 merger trees originating from 40 unique simulations (with 25 trees per simulation), the target variables (`y`) remain constant for all trees derived from the same simulation (`lh_id`). This analysis helps confirm the expected ranges and variability of the target variables.
    \item \textbf{Graph Structural Properties}: Analyzing the distribution of graph sizes, specifically the number of nodes and edges in each merger tree. This is important because the variability in graph size necessitates the use of graph embedding methods that can produce fixed-size representations.
    \item \textbf{Edge Attribute Investigation}: Examining the nature and distribution of the provided `edge_attr`. If this attribute is not directly interpretable as a physical quantity relevant to merger events, we will proceed to compute these features manually as described later.
\end{enumerate}
\citep{kent2017editorialtechniquesmethodsastrophysical,giri2025astronomycalcpythontoolkitteaching}

\subsubsection{Data Splitting}
To ensure robust and unbiased model evaluation, the dataset was split into training and testing sets \citep{frailis2004perspectsastrophysicaldatabases,onose2016scalablesplittingalgorithmsbigdata}. Critically, this split was performed at the simulation level using the `lh_id` to prevent data leakage, as all trees from the same simulation share identical $\Omega_m$ and $\sigma_8$ values. A `GroupShuffleSplit` from scikit-learn was employed to achieve this, ensuring that all trees from a given simulation were assigned to either the training or testing set. We used an 80/20 split, where 80\% of the unique simulations were used for training and the remaining 20\% for testing.

\subsubsection{Feature Normalization}
To improve model performance and stability, node features (`x`) were normalized using `StandardScaler` from scikit-learn \citep{bhowmik2024imageprocessinganalysismultiple}. The mean and standard deviation for each of the four node features were calculated *solely from the training set graphs*. This ensures that the testing set is normalized using statistics derived from the training data, preventing data leakage \citep{różański2021suppnetneuralnetworkstellar}. The normalization transformation was then applied to both the training and testing sets. Target variables `y` ($\Omega_m$, $\sigma_8$) were not normalized, as tree-based regressors like Random Forest and Gradient Boosting are generally insensitive to the scale of the target variable \citep{stonemartinez2025starflowleveragingnormalizingflows}.

\subsection{Feature Engineering from Merger Trees}
For each graph in the dataset, we extracted three distinct sets of features: graph spectral features \citep{jespersen2022textttmangrovelearninggalaxyproperties,bose2022constructinghighfidelityhalomerger}, diffusion geometry embeddings \citep{robles2022deeplearningapproachhalo}, and astrophysically-motivated edge feature aggregations \citep{gómez2021halomergertreecomparison}. Each of these sets aims to capture different aspects of the merger tree structure and the underlying physical processes.

\subsubsection{Graph Spectral Feature Extraction}
Graph spectral features capture the global connectivity and structure of the merger trees by analyzing the eigenvalues of the graph Laplacian \citep{desouza2023graphbasedspectralclassificationtype,pavlou2023graphtheoreticalanalysislocal}.
\begin{enumerate}
    \item \textbf{Normalized Graph Laplacian Computation}: For each graph $g = (V, E)$, we first converted the `edge_index` to a SciPy sparse adjacency matrix $A$. We then computed the normalized graph Laplacian, defined as $L_{norm} = I - D^{-1/2} A D^{-1/2}$, where $D$ is the diagonal degree matrix and $I$ is the identity matrix. The normalization ensures that the eigenvalues of $L_{norm}$ lie in the interval $[0, 2]$, providing a stable basis for spectral analysis.
    \item \textbf{Eigenvalue Decomposition}: We computed the eigenvalues of $L_{norm}$ using the `scipy.sparse.linalg.eigsh` function, which is optimized for sparse matrices. Given the varying sizes of the graphs, storing all eigenvalues for each graph would result in variable-length feature vectors.
    \item \textbf{Spectral Moment Calculation}: To obtain a fixed-size feature vector for each graph, we computed spectral moments from its eigenvalue distribution. Specifically, we calculated the first four central moments: the mean, standard deviation, skewness, and kurtosis of the eigenvalues. Additionally, we calculated the sum of the smallest $m=10$ non-zero eigenvalues (if available, padded with zeros if fewer than 10 non-zero eigenvalues existed). These spectral moments provide a compact summary of the shape of the eigenvalue distribution, capturing key structural properties of the graph.
\end{enumerate}

\subsubsection{Diffusion Geometry Embedding}
Diffusion geometry provides an alternative approach to characterizing graph structure by embedding nodes in a low-dimensional space that reflects the diffusion dynamics on the graph \citep{berger2017visualizingtimevaryingparticleflows,jagvaral2023unifiedframeworkdiffusiongenerative}.
\begin{enumerate}
    \item \textbf{Diffusion Operator Construction}: For each graph, we constructed a diffusion operator using the random walk transition matrix $P = D^{-1}A$, where $D$ is the diagonal degree matrix and $A$ is the adjacency matrix. This matrix represents the probability of transitioning between connected nodes in one step of a random walk.
    \item \textbf{Eigenvector Computation for Node Embeddings}: We computed the eigenvectors of $P$ and selected the top $d$ eigenvectors corresponding to the largest eigenvalues (excluding the trivial eigenvector for eigenvalue 1, if present). We experimented with $d=3$ and $d=5$ to assess the sensitivity of the results to the embedding dimensionality. These eigenvectors form a $d$-dimensional embedding for each node in the graph, capturing its position within the graph's diffusion geometry.
    \item \textbf{Graph-Level Aggregation of Node Embeddings}: To obtain a fixed-size feature vector for each graph, we aggregated the $d$-dimensional node embeddings using mean pooling, max pooling, and min pooling. These operations compute the element-wise mean, maximum, and minimum of the node embeddings, respectively. The resulting vectors were then concatenated to form a single graph-level feature vector of length $3d$. For example, with $d=3$, the resulting feature vector would have a length of 9. This aggregation summarizes the distribution of node positions in the diffusion embedding space, providing a global representation of the graph's structure.
\end{enumerate}

\subsubsection{Astrophysically-Motivated Edge Feature Aggregation}
These features aim to capture relevant aspects of the merger events within the merger trees, focusing on the physical properties of the merging halos \citep{jespersen2022textttmangrovelearninggalaxyproperties,jung2024mergertreebasedgalaxymatching,chandrogómez2025accuracydarkmatterhalo}.
\begin{enumerate}
    \item \textbf{Calculation of Edge-Level Physical Properties}: For each edge $(u, v)$ in a merger tree, representing a progenitor halo $u$ merging into a descendant halo $v$, we calculated the following properties:
    \begin{itemize}
        \item \textbf{Scale Factor Difference}: $\Delta_{sf} = |scale\_factor(u) - scale\_factor(v)|$. Since edges point from progenitors to descendants, we expect $scale\_factor(v) > scale\_factor(u)$, and $\Delta_{sf}$ represents the time interval between the merger event and the observation time.
        \item \textbf{Log Mass Ratio}: $log\_mass\_ratio = log10(mass(v)) - log10(mass(u))$, representing the ratio of the descendant halo mass to the progenitor halo mass. This quantity reflects the mass accretion during the merger event.
    \end{itemize}
    \item \textbf{Aggregation to Graph-Level Statistics}: For each graph, we calculated the mean and variance of $\Delta_{sf}$ and $log\_mass\_ratio$ over all its edges. This resulted in four features per graph: the mean and variance of the scale factor difference, and the mean and variance of the log mass ratio. These statistics provide a summary of the typical and atypical merger event characteristics within a tree.
\end{enumerate}

\subsection{Model Development and Evaluation}
\subsubsection{Feature Vector Assembly and Dimensionality Reduction}
For each graph, we concatenated the features derived from graph spectral moments, aggregated diffusion map embeddings, and aggregated astrophysically-motivated edge statistics \citep{sun2024knowledgegraphastronomicalresearch}. This resulted in a single, high-dimensional feature vector per merger tree. To reduce dimensionality, decorrelate features, and potentially improve model performance, we applied Principal Component Analysis (PCA) to the combined feature vectors derived from the training set \citep{ma2025characterizingdarkmattersubhalo}. The number of principal components to retain was determined by the amount of variance explained (e.g., 95\% or 99\%). Both training and testing set feature vectors were then transformed using the fitted PCA.

\subsubsection{Regression Models for Cosmological Parameter Prediction}
We trained separate regression models to predict $\Omega_m$ and $\sigma_8$ \citep{villaescusanavarro2021camelsprojectcosmologyastrophysics}. We explored two different regression models: Random Forest Regressor and Gradient Boosting Regressor (specifically, XGBoost and LightGBM). These models were chosen for their robustness, ability to capture non-linear relationships, and relatively low computational cost \citep{villaescusanavarro2021camelsprojectcosmologyastrophysics,balla2024cosmicscalebenchmarksymmetrypreservingdata,makinen2024hybridsummarystatistics}.

\subsubsection{Training and Hyperparameter Optimization}
The selected models were trained on the PCA-transformed feature vectors from the training set \citep{steiner2009pcatomographyextractinformation,yun2023pcafilteringmethodunbiased,kuiper2025representationlearningfastradio}. Hyperparameter optimization was performed using cross-validation on the training set. To ensure that the cross-validation process did not introduce data leakage, the cross-validation splits were also performed at the simulation level, respecting the `lh_id` groupings. We employed `GridSearchCV` or `RandomizedSearchCV` to search for the optimal hyperparameter settings for each model.

\subsubsection{Evaluation Metrics}
The performance of the trained models was evaluated on the test set using the following metrics: \citep{mishrasharma2024paperclipassociatingastronomicalobservations,narkedimilli2024predictingstellarmetallicitycomparative,raghav2024photometricanalysispredictingstar}.
\begin{itemize}
    \item \textbf{R-squared (R²)}: The coefficient of determination, which measures the proportion of variance in the target variable that is explained by the model.
    \item \textbf{Mean Squared Error (MSE)}: The average squared difference between the predicted and actual values.
\end{itemize}
If the models provided uncertainty estimates (e.g., quantile regression for Gradient Boosting, or variance from Random Forests), we assessed their calibration using Expected Calibration Error (ECE) \citep{pandya2025siddasinkhorndynamicdomain} and reliability diagrams. These metrics evaluate the consistency between the predicted probabilities/quantiles and the observed frequencies.

\subsubsection{Baseline Models for Comparison}
To provide context for the performance of our proposed feature set, we compared our results against two baseline models \citep{pan2024astromlab2astrollama270bmodel,dehaan2025astromlab3achievinggpt4o}.
\begin{enumerate}
    \item \textbf{Baseline 1: Aggregated Node Features with Classical Regressors}
For each graph, we computed simple aggregated node features: mean, standard deviation, minimum, and maximum for each of the four raw node features (log10(mass), log10(concentration), log10(Vmax), scale factor). This resulted in a 16-dimensional feature vector per graph. We then trained Random Forest and Gradient Boosting regressors on these aggregated features, using the same training/testing split and hyperparameter optimization strategy as before \citep{soo2023machinelearningapplicationsastrophysics}. This baseline tests whether sophisticated graph features outperform simple global statistics of node properties.
    \item \textbf{Baseline 2: Graph Convolutional Network (GCN)}
We implemented a simple Graph Convolutional Network (GCN) for graph-level regression. The GCN architecture consisted of 2-3 `GCNConv` layers with ReLU activations, followed by a global mean pooling layer to obtain a graph-level embedding, and an MLP (2 fully connected layers) to regress $\Omega_m$ and $\sigma_8$ \citep{zhong2024improvingconvolutionalneuralnetworks,kvasiuk2024reconstructioncontinuouscosmologicalfields}. The GCN was trained on the training graph data using the MSE loss function. The training process was performed on CPUs. This baseline compares our "classical" graph feature engineering approach against a common geometric deep learning model.
\end{enumerate}

\end{document}
                