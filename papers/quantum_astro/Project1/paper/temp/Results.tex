\documentclass[twocolumn]{aastex631}

\usepackage{amsmath}
\usepackage{multirow}
\usepackage{natbib}
\usepackage{graphicx} 
\usepackage{aas_macros}

\begin{document}

\section{Results and Discussion}

This section details the outcomes of applying Quantum Tensor Train (QTT) based feature engineering to cosmological merger tree subgraphs for the prediction of final halo mass. It interprets the quantitative performance of regression models, analyzes the characteristics of the derived features, and discusses the implications and limitations of the approach.

\subsection{Data Preprocessing and Subgraph Extraction Yield}

The initial dataset comprised 300 merger trees. Node features (mass, concentration, $v_{max}$, scale factor) underwent a preprocessing pipeline involving a logarithmic transformation for mass and $v_{max}$, followed by standardization (zero mean, unit variance) of all four features based on global statistics from the entire dataset. This ensured numerical stability and feature comparability. For instance, raw halo mass, originally spanning approximately $9.7$ to $14.5$ (log$_{10}$ scale as per problem description), was transformed and standardized to a mean near zero and unit standard deviation.

A critical step involved extracting $k$-hop subgraphs around nodes on the main progenitor branch of each merger tree. The main branch was identified using the \texttt{mask\_main} attribute provided with each tree. However, a significant challenge emerged during this phase: the vast majority of main branch node indices specified in \texttt{mask\_main} were found to be invalid (out of bounds) for their respective trees. This issue drastically limited the number of valid subgraphs that could be extracted. Across all 300 trees and for all tested $k$-values ($k=1, 2, 3$), only a total of 5 unique subgraphs, originating from 5 distinct trees, could be successfully processed.

Consequently, all subsequent QTT feature engineering, model training, and evaluation were performed on this severely reduced dataset of $N=5$ trees. For $k=1$, the extracted subgraphs had an average of 4.0 nodes (min: 3, max: 8). For $k=2$, the average was 8.4 nodes (min: 5, max: 19), and for $k=3$, it was 13.4 nodes (min: 7, max: 32). These subgraphs' node feature matrices were padded to the next power of 2 in the node dimension (e.g., 8 for $k=1$, 32 for $k=2$ and $k=3$) to prepare them for QTT decomposition, as described in the Methods section.

This extremely small effective sample size ($N=5$) is the most significant limitation of the current study. While the methodology was executed as planned, the quantitative results for regression performance must be interpreted with extreme caution, as they are based on in-sample evaluation on these 5 data points and are not generalizable. The findings should be considered a proof-of-concept demonstration on a minimal dataset rather than a robust statistical evaluation.

\subsection{QTT Decomposition and Feature Engineering}

For each of the 5 valid subgraphs, the padded node feature matrix (e.g., shape $[8, 4]$ for $k=1$) was reshaped into a higher-order tensor (e.g., $[2, 2, 2, 4]$ for $k=1$) and decomposed using QTT. Experiments were conducted with QTT ranks of 2 and 3. The QTT cores were then flattened and concatenated to form a single feature vector for each subgraph. For trees with multiple (though in this case, only one per tree) valid main branch subgraphs, these QTT vectors were intended to be aggregated by mean pooling; here, it simply meant taking the QTT vector of the single available subgraph.

The reconstruction Mean Squared Error (MSE) of the QTT decomposition provides an indication of the compression fidelity. For $k=1$ subgraphs, a QTT rank of 2 yielded an average reconstruction MSE of approximately 0.032, while a rank of 3 reduced this to 0.0053. For $k=2$, rank 2 gave an MSE of 0.033, and rank 3 gave 0.016. For $k=3$, rank 2 resulted in an MSE of 0.057, and rank 3 in 0.027. These low MSE values suggest that QTT, even with relatively low ranks, could reconstruct the (padded) subgraph feature matrices with reasonable accuracy, indicating that the compressed QTT features retained substantial information from the local subgraph environments.

\subsection{Regression Performance for Final Halo Mass Prediction}

Random Forest Regressors were trained to predict the first component of the target variable $y$ (representing a final halo mass property at $z=0$), using either baseline aggregated features or the QTT-derived features. Given $N=5$, all evaluations are in-sample.

\subsubsection{Baseline Model}
Baseline features were constructed by taking the mean, maximum, and variance of the four preprocessed node features along the (valid portion of the) main branch for each of the 5 trees. This resulted in a 12-dimensional feature vector per tree.
The baseline model achieved:
\begin{itemize}
    \item Mean Squared Error (MSE): 0.00197
    \item Mean Absolute Error (MAE): 0.0315
    \item R-squared (R²): 0.797
\end{itemize}

\subsubsection{QTT-based Models}
The performance of QTT-based models varied with $k$ and QTT rank, as detailed in Table 1.

\begin{table}[h!]
    \centering
    \caption{Regression performance of QTT-based models}
    \begin{tabular}{c c c c c}
        \hline
        k & QTT Rank & MSE & MAE & R² \\
        \hline
        1 & 2 & 0.00151 & 0.0261 & 0.845 \\
        1 & 3 & 0.00159 & 0.0276 & 0.836 \\
        2 & 2 & 0.00161 & 0.0279 & 0.834 \\
        2 & 3 & 0.00181 & 0.0291 & 0.813 \\
        3 & 2 & 0.00196 & 0.0348 & 0.798 \\
        3 & 3 & 0.00187 & 0.0341 & 0.808 \\
        \hline
    \end{tabular}
\end{table}

\subsubsection{Comparative Analysis}
Numerically, the QTT model with $k=1$, rank=2 showed the best performance (R²=0.845) among all models, slightly outperforming the baseline (R²=0.797) on this $N=5$ dataset. Other QTT configurations also showed R² values comparable to or slightly better than the baseline.

It is crucial to reiterate that with $N=5$, these differences are not statistically significant and are highly susceptible to the specific characteristics of these five samples. The primary takeaway is that the QTT feature engineering pipeline is functional and can produce features usable by a standard regressor. The observed high R² values, while numerically impressive, should not be interpreted as evidence of a generally superior model without validation on a substantially larger and more representative dataset.

\subsection{Feature Space Analysis}

\subsubsection{Feature Distributions}
For the baseline features, the \texttt{*\_var} features (variance of mass, concentration, etc., along the main branch) were all zero for the 5 selected trees. This suggests that for these specific trees, either the valid main branch segment consisted of a single node, or the features were constant along the main branch segment. This is another artifact of the extremely small and potentially unrepresentative sample. The \texttt{*\_mean} and \texttt{*\_max} features showed some variation.

The QTT features, being components of compressed tensor cores, exhibit distributions that are not directly interpretable in terms of physical properties.

\subsubsection{Dimensionality Reduction (PCA)}
For the baseline features, the first two principal components explained approximately 75.7\% and 23.6\% of the variance, respectively (total $\sim$99.3\%). For the QTT features ($k=1$, rank=2), the first two components explained about 71.3\% and 24.0\% of the variance (total $\sim$95.3\%). The high cumulative explained variance in both cases suggests that much of the feature variability within this tiny sample can be captured in a low-dimensional space.

\subsubsection{Feature Importances}
For the $N=5$ sample, baseline features like \texttt{mass\_mean}, \texttt{vmax\_mean}, \texttt{concentration\_mean}, and their \texttt{\_max} counterparts showed non-zero importance. As noted, \texttt{*\_var} features had zero importance because their values were zero.

The QTT feature importance plots show a distribution of importances across the abstract QTT features. For $k=1$, rank=2 (28 features), several features contributed to the prediction. The interpretation of individual QTT feature importances is challenging due to their abstract nature. However, the fact that the model assigns varying importances suggests that different components of the compressed QTT representation contribute differently to the predictive task.

The utility of QTT features lies in their potential to automatically learn and encode complex, non-linear relationships and structural information from the local subgraph environment into a compact vector. This could be more powerful than simple statistical aggregations (mean, max, var) if the local graph structure and multi-feature interactions are important for the prediction task. However, this potential can only be validated with a much larger dataset.

\subsection{Impact of $k$-hop Neighborhood and QTT Rank}

Analyzing the table in Section 3.2:

\subsubsection{Impact of $k$}
For rank 2, performance (R²) decreased slightly as $k$ increased ($k=1$: 0.845, $k=2$: 0.834, $k=3$: 0.798). A similar, though less clear, trend is seen for rank 3. This might suggest that for this tiny dataset, larger subgraphs (larger $k$) introduced more noise or irrelevant information relative to the signal.

\subsubsection{Impact of QTT Rank}
Higher QTT rank allows for less compression and potentially captures more detail. However, this did not consistently translate to better predictive performance on this $N=5$ dataset. This could be due to overfitting with more complex features on such a small sample, or the specific information captured by the higher rank cores not being relevant for these 5 samples.

Again, these trends are based on $N=5$ and are not robust.

\subsection{Limitations and Future Directions Summary}

This study successfully implemented a pipeline for QTT-informed feature engineering on merger trees, demonstrating the feasibility of extracting and compressing information from local subgraph environments. Despite the promising reconstruction accuracy of QTT and the nominally high R² values achieved on the limited dataset, the core limitation lies in the extremely small effective sample size ($N=5$) due to issues with subgraph extraction. This prevents any meaningful conclusions regarding the generalizability or statistical significance of the results.

Future research should focus on resolving the subgraph extraction issue, validating the approach on larger datasets, exploring a wider range of QTT parameters, and comparing the performance against other methods like GNNs. Addressing these limitations will be crucial to realizing the full potential of QTT for feature engineering in cosmological merger tree analysis.

\end{document}
                