\documentclass[twocolumn]{aastex631}

\usepackage{amsmath}
\usepackage{multirow}
\usepackage{natbib}
\usepackage{graphicx} 
\usepackage{aas_macros}

\begin{document}

\section{Methodology}

This section details the methodology employed for QTT-informed subgraph feature engineering on merger trees to predict final halo mass. We describe the dataset used, the preprocessing steps applied, the subgraph extraction process, the QTT-based feature engineering technique, feature aggregation strategies, the regression modeling approach, evaluation metrics, and visualization methods. This approach aims to extract meaningful information from localized subgraphs, addressing the challenges outlined in the introduction by leveraging QTT to capture relevant information while maintaining computational efficiency.

\subsection{Data Acquisition and Preprocessing}

The analysis utilizes a dataset of 300 cosmological merger trees, each representing the hierarchical assembly history of a dark matter halo. These merger trees are stored in PyTorch Geometric format, a library designed for handling graph-structured data. Each tree contains node features, including mass, concentration, maximum circular velocity (\textit{vmax}), and scale factor (\textit{a}), along with edge information indicating the relationships between halos at different redshifts. Furthermore, each tree includes a mask identifying the nodes belonging to the main progenitor branch, which represents the primary lineage of the final halo. The data is loaded using PyTorch, and all subsequent processing is performed on CPUs due to computational constraints.

Prior to feature engineering, node features are preprocessed to ensure numerical stability and comparability. This preprocessing is crucial for the subsequent steps, particularly the QTT decomposition, as it helps to normalize the data and prevent any single feature from dominating the representation. The preprocessing steps are as follows:

\subsubsection{Logarithmic Transformation}
To reduce the dynamic range and approximate normality, the mass and \textit{vmax} features are log-transformed using the natural logarithm:
\[
\text{Mass}_{\text{transformed}} = \log(\text{Mass})
\]
\[
\textit{vmax}_{\text{transformed}} = \log(\textit{vmax})
\]
This transformation helps to mitigate the effects of outliers and ensures that these features are on a more comparable scale.

\subsubsection{Normalization}
All features, including the log-transformed mass and \textit{vmax}, as well as concentration and scale factor, are standardized to have zero mean and unit variance. This is achieved by subtracting the mean and dividing by the standard deviation, computed from the training set:
\[
x_{\text{normalized}} = \frac{x - \mu}{\sigma}
\]
where \(x\) represents a feature, \(\mu\) is the mean of that feature in the training set, and \(\sigma\) is the standard deviation of that feature in the training set. This normalization step is essential for the effective application of QTT decomposition and to prevent any single feature from dominating the representation.

\subsection{Subgraph Extraction}

For each merger tree in the dataset, the main progenitor branch is identified using the provided mask. To capture the local environment and assembly history of each node along the main branch, \(k\)-hop subgraphs are extracted. The \(k\)-hop neighborhood of a node includes all nodes reachable within \(k\) edges from the central node, as well as the connecting edges. The value of \(k\) is a hyperparameter that controls the size of the extracted subgraphs and is selected empirically (e.g., \(k = 1, 2, 3\)) to balance the level of local detail captured with computational tractability. A larger \(k\) captures a more extended assembly history but also increases the computational cost.

For each tree, this process yields a collection of overlapping subgraphs, each centered on a node along the main progenitor branch. Each subgraph is represented by its node feature matrix, where each row corresponds to a node in the subgraph and each column corresponds to a feature (mass, concentration, \textit{vmax}, scale factor).

\subsection{QTT-Based Feature Engineering}

Each subgraph's node feature matrix (with a shape of [number of nodes, 4]) is prepared for Quantum Tensor Train (QTT) decomposition. If necessary, matrices are padded with zeros or reshaped to fit the requirements of the QTT algorithm (e.g., to a power-of-two size). QTT decomposition is then applied to each matrix, producing a sequence of low-rank tensor cores. The QTT decomposition approximates the original matrix as a product of smaller tensors, effectively compressing the information.

The QTT rank, which determines the size of the tensor cores, is a crucial hyperparameter. A lower rank yields more aggressive compression, reducing the dimensionality of the feature vector but potentially losing information. The choice of QTT rank is based on computational feasibility and the desired level of compression, and it is typically chosen through experimentation. These cores are then flattened and concatenated to form a fixed-length QTT-based feature vector for each subgraph. This vector represents a compressed encoding of the local environment and assembly history captured by the subgraph.

\subsection{Feature Aggregation}

Since each merger tree yields multiple QTT feature vectors (one per node along the main progenitor branch), these vectors are aggregated to produce a single feature vector per tree. This aggregation step is necessary to represent the entire merger tree with a single feature vector suitable for regression modeling. Several aggregation strategies are explored:

\subsubsection{Mean Pooling}
This strategy involves averaging the QTT feature vectors across all subgraphs within a tree. The resulting vector represents the average local environment along the main progenitor branch.
\[
\text{Feature Vector} = \frac{1}{N} \sum_{i=1}^{N} \text{QTT Vector}_i
\]
where \(N\) is the number of subgraphs and \(\text{QTT Vector}_i\) is the QTT feature vector for the \(i\)-th subgraph.

\subsubsection{Max Pooling}
This strategy involves taking the element-wise maximum across all QTT feature vectors within a tree. This captures the most salient features present in any of the subgraphs.
\[
\text{Feature Vector} = \max(\text{QTT Vector}_1, \text{QTT Vector}_2, ..., \text{QTT Vector}_N)
\]
where the maximum is taken element-wise.

\subsubsection{Concatenation}
This strategy involves concatenating the QTT feature vectors from a fixed number of nodes along the main progenitor branch (e.g., the last \(n\) nodes, representing the most recent assembly history). If the number of nodes is less than \(n\), the vectors are padded with zeros.

The aggregation method is selected based on empirical performance and interpretability, as different strategies may be more effective at capturing the relevant information for predicting final halo mass.

\subsection{Regression Modeling}

The aggregated QTT-based feature vectors serve as input to a regression model tasked with predicting the final halo mass at \(z=0\) (the mass of the main progenitor at the final snapshot). Given the moderate dataset size and CPU-only environment, ensemble tree-based models such as Random Forests are employed for their robustness, interpretability, and ability to handle non-linear relationships.

The Random Forest regressor consists of an ensemble of decision trees, each trained on a random subset of the data and features. The final prediction is obtained by averaging the predictions of all trees in the ensemble. The hyperparameters of the Random Forest model, such as the number of trees, the maximum depth of the trees, and the minimum number of samples required to split a node, are optimized via cross-validation to prevent overfitting and maximize performance.

Baseline models using traditional features (e.g., mean, max, or variance of node features along the main branch) are also trained for comparison. These baseline models provide a benchmark against which to evaluate the effectiveness of the QTT-based feature engineering approach.

\subsection{Evaluation Metrics}

Model performance is evaluated using the following metrics:

\subsubsection{Mean Squared Error (MSE)}
The MSE measures the average squared difference between the predicted and true values:
\[
\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2
\]
where \(y_i\) is the true value, \(\hat{y}_i\) is the predicted value, and \(N\) is the number of samples.

\subsubsection{Mean Absolute Error (MAE)}
The MAE measures the average absolute difference between the predicted and true values:
\[
\text{MAE} = \frac{1}{N} \sum_{i=1}^{N} |y_i - \hat{y}_i|
\]

\subsubsection{Coefficient of Determination (RÂ²)}
The \(R^2\) measures the proportion of variance in the dependent variable that is predictable from the independent variables:
\[
R^2 = 1 - \frac{\sum_{i=1}^{N} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{N} (y_i - \bar{y})^2}
\]
where \(\bar{y}\) is the mean of the true values.

These metrics provide a comprehensive assessment of the model's predictive performance.

\subsection{Visualization and Interpretation}

A comprehensive suite of visualizations is produced to assess the effectiveness and interpretability of QTT-based features:

\subsubsection{Feature Distributions}
Histograms and density plots of QTT features are compared to those of traditional features. This allows for a visual assessment of the distribution and range of values for the different feature sets.

\subsubsection{Regression Performance}
Scatter plots of predicted vs. true halo mass, residual plots (predicted - true), and bar charts of performance metrics (MSE, MAE, \(R^2\)) are generated for both QTT-based and baseline models. These plots provide a visual comparison of the predictive performance of the different models.

\subsubsection{Feature Importance}
Rankings of QTT feature importances are obtained from the tree-based models. These rankings indicate the relative importance of each QTT feature in predicting the final halo mass. Correlation heatmaps between QTT features and the target variable are also generated to identify the features that are most strongly correlated with halo mass.

\subsubsection{Physical Interpretation}
Selected subgraphs are visualized with QTT feature overlays to gain insight into the physical meaning of the QTT-compressed features. Dimensionality reduction techniques (e.g., PCA or t-SNE) are applied to the QTT feature space, and the resulting low-dimensional representations are colored by halo mass to visualize the relationship between QTT features and halo mass.

\subsection{Addressing Implementation Challenges}

Several challenges were encountered and addressed during implementation:

\subsubsection{Computational Constraints}
QTT decomposition can be computationally expensive, especially for large matrices. To ensure tractability on CPUs, QTT was applied only to small, localized subgraphs. Efficient batching and memory management were employed to optimize performance.

\subsubsection{Feature Matrix Size Variability}
Subgraphs can have varying sizes, depending on the local environment of the node. To ensure compatibility with QTT decomposition, subgraphs of varying sizes were padded with zeros or truncated to a fixed size.

\subsubsection{Hyperparameter Selection}
The choice of \(k\) (subgraph size), QTT rank, and aggregation method is crucial for performance. These hyperparameters were optimized through cross-validation and empirical performance evaluation.

\subsubsection{Interpretability}
Interpreting the physical meaning of QTT-compressed features can be challenging. Visualization techniques and feature importance analysis were used to gain insight into the meaning of these features.

This methodology enables efficient, interpretable, and physically motivated feature engineering for merger tree data, leveraging QTT to extract salient information for predictive modeling of halo properties.

\end{document}
                